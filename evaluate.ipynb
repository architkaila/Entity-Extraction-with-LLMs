{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truths(json_data):\n",
    "    \"\"\"\n",
    "    Parse the ground truths from the provided JSON data.\n",
    "\n",
    "    Args:\n",
    "    - json_data: A dictionary representing the JSON data.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the extracted entities.\n",
    "    \"\"\"\n",
    "    output = json.loads(json_data[\"output\"])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_response(response):\n",
    "    \"\"\"\n",
    "    Parse the model response to extract entities.\n",
    "\n",
    "    Args:\n",
    "    - response: A string representing the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the extracted entities.\n",
    "    \"\"\"\n",
    "    response = response.replace(\"### Response:\", \"\")\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(ground_truths, predictions):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall.\n",
    "\n",
    "    Args:\n",
    "    - ground_truths: A list of dictionaries, where each dictionary contains the ground truth entities.\n",
    "    - predictions: A list of dictionaries, where each dictionary contains the predicted entities.\n",
    "\n",
    "    Returns:\n",
    "    - precision: The precision of the predictions.\n",
    "    - recall: The recall of the predictions.\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for gt, pred in zip(ground_truths, predictions):\n",
    "        gt_entities = set(sum(gt.values(), [])) # Flatten the values list\n",
    "        pred_entities = set(sum(pred.values(), [])) # Flatten the values list\n",
    "\n",
    "        true_positives += len(gt_entities & pred_entities)\n",
    "        false_positives += len(pred_entities - gt_entities)\n",
    "        false_negatives += len(gt_entities - pred_entities)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (replace this with your actual data)\n",
    "json_data = {\n",
    "    \"input\": \"...\",\n",
    "    \"output\": \"{\\\"drug_name\\\": \\\"Abilify\\\", \\\"adverse_events\\\": [\\\"nausea\\\", \\\"vomiting\\\"]}\"\n",
    "}\n",
    "model_response = \"### Response:{\\\"drug_name\\\": \\\"Abilify\\\", \\\"adverse_events\\\": [\\\"nausea\\\", \\\"vomiting\\\"]}\"\n",
    "\n",
    "ground_truth = parse_ground_truths(json_data)\n",
    "prediction = parse_model_response(model_response)\n",
    "\n",
    "precision, recall = calculate_precision_recall([ground_truth], [prediction])\n",
    "print(f\"Precision: {precision}, Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
